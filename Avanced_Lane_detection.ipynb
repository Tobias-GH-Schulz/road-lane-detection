{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00000-672337fb-0178-41f6-9487-3ebcab142c49",
        "deepnote_cell_type": "code"
      },
      "source": "import cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib.patches import Polygon\nimport numpy as np\nimport glob\n\nfrom moviepy.editor import VideoFileClip\nfrom IPython.display import HTML\n\n%matplotlib inline\n\n# list of test images' paths\ntest_in_paths = glob.glob('test_images/*')\n\n\ndef plot_result(imgs, names, rows=0, cols=0):\n    '''\n    plot the result\n    '''\n    if (len(imgs)==0 or len(names)==0): \n        return -1\n    \n    f, ax = plt.subplots(rows, cols, figsize=(16,8))\n    f.tight_layout()\n    i = 0\n    if rows <= 1 :\n        for c in range(cols):\n            ax[c].imshow(imgs[i], cmap='gray')\n            ax[c].set_title('{}'.format(names[i]), fontsize=24)\n            ax[c].axis('off')\n            i += 1\n    else:\n        for r in range(rows):\n            for c in range(cols):\n                ax[r, c].imshow(imgs[i], cmap='gray')\n                ax[r, c].set_title('{}'.format(names[i]), fontsize=24)\n                ax[r, c].axis('off')\n                i += 1\n    plt.suptitle(p.split('/')[-1], fontsize=36)\n    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n    \n\n\ndef camera_calibration(cal_path, debug=False):\n    '''\n    this routine performs camera calibration\n    it returns `mtx` and `dist` needed to\n    undistort images taken from this camera\n    '''\n    # list all calibration images paths\n    cal_images_names = glob.glob(cal_path)\n\n    # chessboard-specific parameters\n    nx = 9\n    ny = 6\n\n    # code below is based on classroom example\n    objpoints = [] # 3D points\n    imgpoints = [] # 2D points\n\n    # (x,y,z): (0,0,0), (1,0,0), etc\n    objp = np.zeros((nx * ny,3), np.float32)\n    objp[:,:2] = np.mgrid[0:nx,0:ny].T.reshape(-1,2) # x, y coordinates, z stays 0\n\n    for fname in cal_images_names:\n        # read in image\n        img = cv2.imread(fname)\n\n        # convert to grayscale\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n        # find chessboard corners\n        ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n\n        # in case chessboard was found successfully\n        # it skips 3 images that do not show full chessboard (1, 4 and 5)\n        if ret == True:\n            # image points will be different for each calibration image\n            imgpoints.append(corners)\n            # object points are the same for all calibration images\n            objpoints.append(objp)\n\n            # Draw and display the corners\n            cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n            if debug:\n                plt.figure(figsize=(15,10))\n                plt.imshow(img)\n\n    # calibration parameters calculation\n    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, \n                                                       imgpoints, \n                                                       gray.shape[::-1], \n                                                       None, None)\n\n    # will only use `mtx` and `dist` in this project, hence return\n    return mtx, dist\n\ndef undistort_image(image, mtx, dist):\n    '''\n    returns an undistorted image (after camera calibration)\n    '''\n    dst = cv2.undistort(image, mtx, dist, None, mtx)\n\n    return dst\n\ndef gaussian_blur(image, kernel=5):\n    '''\n    this routine applies blur to reduce noise in images\n    '''\n    blurred = cv2.GaussianBlur(image, (kernel,kernel), 0)\n    return blurred\n\n\ndef get_thresholded_image(img):\n    \n    img = cv2.undistort(img, mtx, dist, None, mtx)\n    \n    # convert to gray scale\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    \n    height, width = gray.shape\n    \n    # apply gradient threshold on the horizontal gradient\n    sx_binary = abs_sobel_thresh(gray, 'x', 10, 200)\n    \n    # apply gradient direction threshold so that only edges closer to vertical are detected.\n    dir_binary = dir_threshold(gray, thresh=(np.pi/6, np.pi/2))\n    \n    # combine the gradient and direction thresholds.\n    combined_condition = ((sx_binary == 1) & (dir_binary == 1))\n    \n    # R & G thresholds so that yellow lanes are detected well.\n    color_threshold = 150\n    R = img[:,:,0]\n    G = img[:,:,1]\n    color_combined = np.zeros_like(R)\n    r_g_condition = (R > color_threshold) & (G > color_threshold)\n    \n    \n    # color channel thresholds\n    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    S = hls[:,:,2]\n    L = hls[:,:,1]\n    \n    # S channel performs well for detecting bright yellow and white lanes\n    s_thresh = (100, 255)\n    s_condition = (S > s_thresh[0]) & (S <= s_thresh[1])\n    \n    # We put a threshold on the L channel to avoid pixels which have shadows and as a result darker.\n    l_thresh = (120, 255)\n    l_condition = (L > l_thresh[0]) & (L <= l_thresh[1])\n\n    # combine all the thresholds\n    # A pixel should either be a yellowish or whiteish\n    # And it should also have a gradient, as per our thresholds\n    color_combined[(r_g_condition & l_condition) & (s_condition | combined_condition)] = 1\n    \n    # apply the region of interest mask\n    mask = np.zeros_like(color_combined)\n    region_of_interest_vertices = np.array([[0,height-1], [width/2, int(0.5*height)], [width-1, height-1]], dtype=np.int32)\n    cv2.fillPoly(mask, [region_of_interest_vertices], 1)\n    thresholded = cv2.bitwise_and(color_combined, mask)\n    \n    return thresholded\n \n    \ndef abs_sobel_thresh(gray, orient='x', thresh_min=0, thresh_max=255):\n    if orient == 'x':\n        sobel = cv2.Sobel(gray, cv2.CV_64F, 1, 0)\n    else:\n        sobel = cv2.Sobel(gray, cv2.CV_64F, 0, 1)\n    abs_sobel = np.absolute(sobel)\n    max_value = np.max(abs_sobel)\n    binary_output = np.uint8(255*abs_sobel/max_value)\n    threshold_mask = np.zeros_like(binary_output)\n    threshold_mask[(binary_output >= thresh_min) & (binary_output <= thresh_max)] = 1\n    return threshold_mask\n\n\ndef dir_threshold(gray, sobel_kernel=3, thresh=(0, np.pi/2)):\n    # Take the gradient in x and y separately\n    sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n    sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n    # 3) Take the absolute value of the x and y gradients\n    abs_sobel_x = np.absolute(sobel_x)\n    abs_sobel_y = np.absolute(sobel_y)\n    # 4) Use np.arctan2(abs_sobely, abs_sobelx) to calculate the direction of the gradient\n    direction = np.arctan2(abs_sobel_y,abs_sobel_x)\n    direction = np.absolute(direction)\n    # 5) Create a binary mask where direction thresholds are met\n    mask = np.zeros_like(direction)\n    mask[(direction >= thresh[0]) & (direction <= thresh[1])] = 1\n    # 6) Return this mask as your binary_output image\n    return mask\n\n\n\ndef warp(img, src_coordinates=None, dst_coordinates=None):\n\n    # Define calibration box in source (original) and destination (desired or warped) coordinates\n    img_size = (img.shape[1], img.shape[0])\n    \n    \n    if src_coordinates is None:\n        src_coordinates = np.float32(\n            [[280,  700],  # Bottom left\n             [595,  460],  # Top left\n             [725,  460],  # Top right\n             [1125, 700]]) # Bottom right\n        \n    if dst_coordinates is None:\n        dst_coordinates = np.float32(\n            [[250,  720],  # Bottom left\n             [250,    0],  # Top left\n             [1065,   0],  # Top right\n             [1065, 720]]) # Bottom right   \n\n    # Compute the perspective transfor, M\n    M = cv2.getPerspectiveTransform(src_coordinates, dst_coordinates)\n\n    \n    # Compute the inverse perspective transfor also by swapping the input parameters\n    Minv = cv2.getPerspectiveTransform(dst_coordinates, src_coordinates)\n    \n    # Create warped image - uses linear interpolation\n    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n\n    return warped, M, Minv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\n# camera calibration step\n\nmtx, dist = camera_calibration('./camera_cal/calibration*.jpg',debug = True)",
      "metadata": {
        "tags": [],
        "cell_id": "00001-15815348-7e52-456d-b11a-0a8157d8ac7e",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def detect_lines(img, return_img=False):\n    # Take a histogram of the bottom half of the image\n    \n    histogram, leftx_base, rightx_base = find_histogram_peaks(img)\n\n    if return_img:\n        # Create an output image to draw on and  visualize the result\n        out_img = np.dstack((img, img, img))*255\n\n    # Choose the number of sliding windows\n    nwindows = 9\n    \n    # Set height of windows\n    window_height = np.int(img.shape[0]//nwindows)\n    \n    # Identify the x and y positions of all nonzero pixels in the image\n    nonzero = img.nonzero()\n    nonzerox = np.array(nonzero[1])   \n    nonzeroy = np.array(nonzero[0])\n   \n\n    # Current positions to be updated for each window\n    leftx_current = leftx_base\n    rightx_current = rightx_base\n    \n    # Set the width of the windows +/- margin\n    margin = 100\n\n    # Set minimum number of pixels found to recenter window\n    minpix = 50\n    \n    # Create empty lists to receive left and right lane pixel indices\n    left_lane_inds = []\n    right_lane_inds = []\n\n    # Step through the windows one by one\n    for window in range(nwindows):\n        # Identify window boundaries in x and y (and right and left)\n        win_y_low = img.shape[0] - (window + 1)*window_height\n        win_y_high = img.shape[0] - window*window_height\n        win_xleft_low = leftx_current - margin\n        win_xleft_high = leftx_current + margin\n        win_xright_low = rightx_current - margin\n        win_xright_high = rightx_current + margin\n        \n        if return_img:\n            # Draw the windows on the visualization image\n            cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high), (0,255,0), 3) \n            cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high), (0,255,0), 3) \n\n        # Identify the nonzero pixels in x and y within the window\n        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n                          (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n                           (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n        # Append these indices to the lists\n        left_lane_inds.append(good_left_inds)\n        right_lane_inds.append(good_right_inds)\n        \n        # If you found > minpix pixels, recenter next window on their mean position\n        if len(good_left_inds) > minpix:\n            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n        if len(good_right_inds) > minpix:        \n            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n\n    # Concatenate the arrays of indices\n    left_lane_inds = np.concatenate(left_lane_inds)\n    right_lane_inds = np.concatenate(right_lane_inds)\n    \n    # Extract left and right line pixel positions\n    leftx = nonzerox[left_lane_inds]\n    lefty = nonzeroy[left_lane_inds] \n    rightx = nonzerox[right_lane_inds]\n    righty = nonzeroy[right_lane_inds] \n\n    # Fit a second order polynomial to each\n    left_fit = np.polyfit(lefty, leftx, 2)\n    right_fit = np.polyfit(righty, rightx, 2)\n    \n    ploty = np.linspace(0, img.shape[0] - 1, img.shape[0])\n    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n    \n    if return_img:\n        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n\n        # Draw left and right lines\n        for index in range(img.shape[0]):\n            cv2.circle(out_img, (int(left_fitx[index]), int(ploty[index])), 3, (255,255,0))\n            cv2.circle(out_img, (int(right_fitx[index]), int(ploty[index])), 3, (255,255,0))\n            \n        return (left_fit, right_fit), (left_fitx, ploty), (right_fitx, ploty), out_img\n\n    return (left_fit, right_fit), (left_fitx, ploty), (right_fitx, ploty)",
      "metadata": {
        "tags": [],
        "cell_id": "00002-68b0b746-e960-4a11-abd7-7bde985d80b8",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "class Pipeline:\n    def __init__(self, cal_path):\n        # Make a list of calibration images\n\n        # Calibrate camera\n        self.mtx, self.dist = camera_calibration(cal_path)\n        \n        # Reinitialize \n        self.lines_fit = None\n        self.past_good_left_lines = []\n        self.past_good_right_lines = []\n        self.running_mean_difference_between_lines = 0\n\n    def __call__(self, img):\n        \n        # STEP 1: UNDISTORT (using camera calibration step matrix and dist)\n        undistorted = undistort_image(img, self.mtx, self.dist)\n        \n        # STEP 2: GAUSSIAN BLUR\n        blurred = gaussian_blur(undistorted, kernel=3)\n\n        # STEP 3: APPLY COLOR SPACE TRANSFORM AND SOBEL THRESHOLDING\n        combined = get_thresholded_image(blurred)\n        \n        # Apply a perspective transform to rectify binary image (\"birds-eye view\")\n        src_coordinates = np.float32(\n            [[280,  700],  # Bottom left\n             [595,  460],  # Top left\n             [725,  460],  # Top right\n             [1125, 700]]) # Bottom right\n\n        dst_coordinates = np.float32(\n            [[250,  720],  # Bottom left\n             [250,    0],  # Top left\n             [1065,   0],  # Top right\n             [1065, 720]]) # Bottom right   \n\n        # STEP 4: WARP BINARY IMAGE INTO TOP-DOWN VIEW\n        warped, M, Minv = warp(combined, src_coordinates, dst_coordinates)\n        \n        \n        # STEP 5: Detect lane pixels and fit to find the lane boundary\n        self.lines_fit, left_points, right_points, out_img = detect_similar_lines(warped, \n                                                                                  self.lines_fit, \n                                                                                  self.past_good_left_lines,\n                                                                                  self.past_good_right_lines,\n                                                                                  self.running_mean_difference_between_lines,\n                                                                                  return_img=True,)\n\n        # STEP 6: Warp the detected lane boundaries back onto the original image.\n        img_lane = draw_lane(img, warped, left_points, right_points, Minv)\n            \n        # STEP 7: Add metrics to the output img\n        out_img = add_metrics(img_lane, leftx=left_points[0], rightx=right_points[0])\n            \n        return out_img",
      "metadata": {
        "tags": [],
        "cell_id": "00003-4b2593cf-cbb8-4176-b4fd-a2c1cf50c943",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def apply_video(input_video, output_video):\n\n    # Process video frames with our 'process_image' function\n    process_image = Pipeline('./camera_cal/calibration*.jpg')\n    \n    ## You may uncomment the following line for a subclip of the first 5 seconds\n    #clip1 = VideoFileClip(input_video).subclip(0,5)\n    clip1 = VideoFileClip(input_video)\n    white_clip = clip1.fl_image(process_image)\n    %time white_clip.write_videofile(output_video, audio=False)\n    \n    print(\"Apply video pipeline: SUCCESS!\")",
      "metadata": {
        "tags": [],
        "cell_id": "00004-601235ea-1d28-4e44-b093-1dc7cfff4cd5",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=86e63d07-a61a-4167-874f-18d75be0a900' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "ef21a8ec-c7eb-4004-a1a7-8ccccad95be7",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}